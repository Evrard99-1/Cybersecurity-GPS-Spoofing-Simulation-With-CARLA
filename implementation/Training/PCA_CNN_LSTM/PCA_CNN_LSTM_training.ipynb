{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Optional: Torch imports if training the model in PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torch.optim as optim\n",
    "    torch_available = True\n",
    "except ImportError:\n",
    "    torch_available = False\n",
    "\n",
    "# ==== CONFIG ====\n",
    "CSV_PATH = '/content/drive/MyDrive/thesis/combined_data_Auto_pilot.csv'  # adjust path if needed\n",
    "SEQUENCE_LENGTH = 10\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "N_FOLDS = 10\n",
    "RANDOM_STATE = 42\n",
    "MODEL_SAVE_PATH = 'cnn1dlstm_spoof_detector.pth'\n",
    "DEVICE = torch.device('cuda' if torch_available and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ==== 1) Load & prepare data ====\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "feature_cols = [c for c in df.columns if c not in ('frame_id', 'image_num', 'label')]\n",
    "df['y'] = (df['label'] == 'spoofed').astype(int)\n",
    "y = df['y'].values\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# ==== 2) Spearman feature filtering ====\n",
    "rhos, pvals = {}, {}\n",
    "for i, col in enumerate(feature_cols):\n",
    "    rho, p = spearmanr(X[:, i], y)\n",
    "    rhos[col] = abs(rho)\n",
    "    pvals[col] = p\n",
    "K = min(36, len(feature_cols))\n",
    "selected = sorted(rhos, key=lambda k: rhos[k], reverse=True)[:K]\n",
    "print(f\"Selected features ({len(selected)}): {selected}\")\n",
    "X_sel = df[selected].values\n",
    "\n",
    "# ==== 3) SMOTE balancing ====\n",
    "smote = SVMSMOTE(random_state=RANDOM_STATE)\n",
    "X_bal, y_bal = smote.fit_resample(X_sel, y)\n",
    "print(f\"Balanced dataset: {np.bincount(y_bal)}\")\n",
    "\n",
    "# ==== 4) Standardize & PCA ====\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_bal)\n",
    "n_components = min(15, X_std.shape[1])\n",
    "pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "print(f\"PCA reduced to {n_components} components, explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# ==== 5) Build sequences ====\n",
    "def build_sequences(X_features, y_labels, seq_len):\n",
    "    seq_X, seq_y = [], []\n",
    "    for i in range(len(X_features) - seq_len):\n",
    "        seq_X.append(X_features[i:i + seq_len])\n",
    "        seq_y.append(y_labels[i + seq_len])\n",
    "    return np.stack(seq_X), np.array(seq_y)\n",
    "\n",
    "X_seq, y_seq = build_sequences(X_pca, y_bal, SEQUENCE_LENGTH)\n",
    "print(f\"Sequence data shapes: {X_seq.shape}, {y_seq.shape}\")\n",
    "\n",
    "# ==== 6) Dataset & DataLoader ====\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ==== 7) Model definition (PyTorch) ====\n",
    "class CNN1DLSTM(nn.Module):\n",
    "    def __init__(self, feat_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=feat_dim, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = self.pool2(self.relu(self.conv2(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n",
    "\n",
    "# ==== 8) Cross-validation training & evaluation ====\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "metrics = {'acc': [], 'prec': [], 'rec': [], 'f1': []}\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_seq)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "    X_train, X_test = X_seq[train_idx], X_seq[test_idx]\n",
    "    y_train, y_test = y_seq[train_idx], y_seq[test_idx]\n",
    "\n",
    "    train_ds = TimeSeriesDataset(X_train, y_train)\n",
    "    test_ds = TimeSeriesDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = CNN1DLSTM(feat_dim=n_components, seq_len=SEQUENCE_LENGTH).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            logits = model(X_batch.to(DEVICE))\n",
    "            preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "            trues.extend(y_batch.numpy())\n",
    "\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    prec = precision_score(trues, preds)\n",
    "    rec = recall_score(trues, preds)\n",
    "    f1 = f1_score(trues, preds)\n",
    "    print(f\"Fold {fold+1} -> Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    metrics['acc'].append(acc)\n",
    "    metrics['prec'].append(prec)\n",
    "    metrics['rec'].append(rec)\n",
    "    metrics['f1'].append(f1)\n",
    "\n",
    "# ==== 9) Summary ====\n",
    "print(\"\\n=== Cross-Validation Results ===\")\n",
    "print(f\"Avg Accuracy: {np.mean(metrics['acc']):.4f} ± {np.std(metrics['acc']):.4f}\")\n",
    "print(f\"Avg Precision: {np.mean(metrics['prec']):.4f} ± {np.std(metrics['prec']):.4f}\")\n",
    "print(f\"Avg Recall: {np.mean(metrics['rec']):.4f} ± {np.std(metrics['rec']):.4f}\")\n",
    "print(f\"Avg F1-score: {np.mean(metrics['f1']):.4f} ± {np.std(metrics['f1']):.4f}\")\n",
    "\n",
    "# ==== 10) Train final model on full data and save ====\n",
    "full_ds = TimeSeriesDataset(X_seq, y_seq)\n",
    "full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "final_model = CNN1DLSTM(feat_dim=n_components, seq_len=SEQUENCE_LENGTH).to(DEVICE)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nTraining final model on full dataset...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    final_model.train()\n",
    "    losses = []\n",
    "    for X_batch, y_batch in full_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        logits = final_model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "# Save the trained final model\n",
    "torch.save(final_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
